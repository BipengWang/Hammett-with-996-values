{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'data1',\n",
    "               'data_file'   : 'ndonor_smiles2.csv'}\n",
    "N_train = 10\n",
    "N_val   = 0\n",
    "N_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(fp_length=256,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=2,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=1,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 10\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 1 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        #print(loss_fun,pred_fun,conv_parser)\n",
    "        #print(loss_fun)\n",
    "        #print(len(test_predictions))\n",
    "        return rmse(test_predictions, test_targets)\n",
    "    \n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "#     print \"Starting Morgan fingerprint experiment...\"\n",
    "#     test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    #print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "    print \"Neural test RMSE:\", test_loss_neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'data1', 'data_file': 'ndonor_smiles2.csv'}\n",
      "\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 65641\n",
      "max of weights 0.08886963478129936\n",
      "SSSSSSSSSSSSS\n",
      "[[0.03483125 0.03542363 0.03548654 ... 0.03488469 0.03490846 0.0353605 ]\n",
      " [0.02320162 0.023821   0.02347452 ... 0.02319115 0.0232966  0.0237105 ]\n",
      " [0.01156752 0.01161639 0.01168786 ... 0.01150779 0.01159498 0.0118248 ]\n",
      " ...\n",
      " [0.05914272 0.05953011 0.05983048 ... 0.05879117 0.05838995 0.05839163]\n",
      " [0.10594991 0.10618733 0.10733293 ... 0.10518865 0.10477244 0.1056708 ]\n",
      " [0.09364058 0.09403366 0.09534422 ... 0.09390623 0.0933652  0.09410361]]\n",
      "10\n",
      "SSSSSSSSSSSSS\n",
      "Iteration 0 loss 1.00119675803965 train RMSE 2.8754606063873243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/autograd/numpy/numpy_vjps.py:444: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return lambda g: g[idxs]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'atom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a1d2e57a8b32>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#     test_loss_morgan = run_morgan_experiment()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Starting neural fingerprint experiment...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtest_loss_neural\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_conv_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a1d2e57a8b32>\u001b[0m in \u001b[0;36mrun_conv_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mnum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         predict_func, trained_weights, conv_training_curve =             train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n\u001b[0;32m---> 34\u001b[0;31m                      train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(loss_fun,pred_fun,conv_parser)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a48668083ade>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed, validation_smiles, validation_raw_targets)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Optimize weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n\u001b[0;32m---> 28\u001b[0;31m                            num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/optimizers.pyc\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m      \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m  \u001b[0;31m# First  moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# Second moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a48668083ade>\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(weights, iter)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;34m\"train RMSE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_raw_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_print_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_smiles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mvalidation_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundo_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Validation RMSE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_raw_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/build_vanilla_net.pyc\u001b[0m in \u001b[0;36mpred_fun\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mfingerprints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfingerprint_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SSSSSSSSSSSSS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36moutput_layer_fun\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_layer_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_layer_fun_and_atom_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36moutput_layer_fun_and_atom_activations\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\"Computes layer-wise convolution, and returns a fixed-size output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0marray_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_rep_from_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0matom_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'atom_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mbond_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bond_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/util.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36marray_rep_from_smiles\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marray_rep_from_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;34m\"\"\"Precompute everything we need from MolGraph so that we can free the memory asap.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmolgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_from_smiles_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     arrayrep = {'atom_features' : molgraph.feature_array('atom'),\n\u001b[1;32m    116\u001b[0m                 \u001b[0;34m'bond_features'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmolgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bond'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/mol_graph.pyc\u001b[0m in \u001b[0;36mgraph_from_smiles_tuple\u001b[0;34m(smiles_tuple)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# This sorting allows an efficient (but brittle!) indexing later on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mbig_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_nodes_by_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'atom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbig_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blair/anaconda3/envs/py2.7/lib/python2.7/site-packages/neuralfingerprint/mol_graph.pyc\u001b[0m in \u001b[0;36msort_nodes_by_degree\u001b[0;34m(self, ntype)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msort_nodes_by_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnodes_by_degree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mntype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mnodes_by_degree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'atom'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2.7] *",
   "language": "python",
   "name": "conda-env-py2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
